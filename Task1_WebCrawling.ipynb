{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RijalBijay/Information_Retrieval_Python_Project/blob/main/Task1_WebCrawling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pharmaceutical-ratio",
      "metadata": {
        "id": "pharmaceutical-ratio"
      },
      "source": [
        "# Information Retrieval Coursework (STW7071CEM)\n",
        "\n",
        "Task 1: Search Engine\n",
        "Create a vertical search engine comparable to Google Scholar, but specialized in retrieving just papers/books published by a member of Coventry University's Research Centre for health and life sciences (RCHL)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "amateur-simple",
      "metadata": {
        "id": "amateur-simple"
      },
      "source": [
        "## Installs Required Packages and Import them\n",
        "If you're using Python along with Beautiful Soup to crawl data from websites in Google Colab, you'll need to install the necessary packages and import them into your Colab notebook.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x22Ckil_rV-",
        "outputId": "65b7198b-f595-466f-bfaa-3bf54dbabc0d"
      },
      "id": "-x22Ckil_rV-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "static-registration",
      "metadata": {
        "id": "static-registration"
      },
      "source": [
        "##Required Python Packages for Web Crawling\n",
        "\n",
        "Package Uses:\n",
        "  Scrapy: A powerful web crawling framework for extracting data from websites.\n",
        "  Requests: A simple HTTP library for making HTTP requests in Python.\n",
        "  BeautifulSoup4: A library for parsing HTML and XML documents, commonly used for web scraping tasks.\n",
        "  NLTK: Natural Language Toolkit, a library for natural language processing tasks such as tokenization, stemming, tagging, parsing, and more.\n",
        "  Gensim: A Python library for topic modeling, document indexing, and similarity retrieval with large corpora.\n",
        "  XGBoost: An optimized distributed gradient boosting library designed for efficiency, flexibility, and portability.\n",
        "  Pandastable: A GUI (Graphical User Interface) widget for displaying and analyzing dataframes in Python using Pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "external-guatemala",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:58:01.977307Z",
          "start_time": "2021-08-01T00:58:01.962437Z"
        },
        "id": "external-guatemala"
      },
      "outputs": [],
      "source": [
        "# To install the below packages, remove the '#'\n",
        "\n",
        "# !pip install scrapy\n",
        "# !pip install requests\n",
        "# !pip install BeautifulSoup4\n",
        "# !pip install nltk\n",
        "# !pip install gensim\n",
        "# !pip install xgboost\n",
        "# !pip install pandastable"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imported the installed packages in the below code\n",
        "\n"
      ],
      "metadata": {
        "id": "NmTNm06PDNgI"
      },
      "id": "NmTNm06PDNgI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "latin-pioneer",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:58:06.417024Z",
          "start_time": "2021-08-01T00:58:01.980987Z"
        },
        "id": "latin-pioneer"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "import datetime\n",
        "import string\n",
        "import json\n",
        "import nltk\n",
        "nltk.download('omw-1.4');\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "virgin-capitol",
      "metadata": {
        "id": "virgin-capitol"
      },
      "source": [
        "# 1. Information Retrieval Engine or Crawler Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "million-marks",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:58:06.423826Z",
          "start_time": "2021-08-01T00:58:06.420176Z"
        },
        "id": "million-marks"
      },
      "outputs": [],
      "source": [
        "# The initial URL or starting point for web crawling or scraping, acting as the entry point for exploring a website.\"\n",
        "URL = \"https://pureportal.coventry.ac.uk/en/organisations/coventry-university/persons/\"\n",
        "\n",
        "#This link provides access to profile pages of individuals associated with Coventry University.\n",
        "profile_url = \"https://pureportal.coventry.ac.uk/en/persons/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "formal-shift",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:58:09.277766Z",
          "start_time": "2021-08-01T00:58:06.428114Z"
        },
        "id": "formal-shift",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "040cd2df-4a07-4c10-8e30-9291ebd2b0a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "def retrieve_max_page_number():\n",
        "\n",
        "    first = requests.get(URL)\n",
        "    soup = BeautifulSoup(first.text, 'html.parser')\n",
        "    final_page = soup.select('#main-content > div > section > nav > ul > li:nth-child(12) > a')[0]['href']\n",
        "    fp = final_page.split('=')[-1]\n",
        "    return int(fp)\n",
        "\n",
        "max = retrieve_max_page_number()\n",
        "max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "hidden-sugar",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:58:09.407285Z",
          "start_time": "2021-08-01T00:58:09.280073Z"
        },
        "id": "hidden-sugar"
      },
      "outputs": [],
      "source": [
        "def verify_department(researcher):\n",
        "\n",
        "    l1 = researcher.find('div', class_='rendering_person_short')\n",
        "\n",
        "    for span in l1.find_all('span'):\n",
        "        # Check department\n",
        "        #print(span.text)\n",
        "        if span.text == str('Centre for Health and Life Sciences'):\n",
        "            name = researcher.find('h3', class_='title').find('span').text\n",
        "            return name\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "def generate_csv_file():\n",
        "     database = pd.DataFrame(columns=['Title', 'Authors', 'Publication Year', 'Publication Link'])\n",
        "     database.to_csv('Crawling_database.csv')\n",
        "\n",
        "def append_to_csv(database):\n",
        "    current_data = pd.read_csv(database, index_col=\"Unnamed: 0\")\n",
        "    return current_data\n",
        "\n",
        "def enter_each_researchers_publication(researcher, url, df):\n",
        "\n",
        "    new_url = url + str(researcher).replace(' ','-').lower() + '/publications/'\n",
        "    page = requests.get(new_url)\n",
        "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "    results = soup.find(id=\"main-content\")\n",
        "    papers = results.find_all(\"li\", class_=\"list-result-item\")\n",
        "\n",
        "    for paper in papers:\n",
        "        title = paper.find('h3', class_='title')\n",
        "        if title is not None:\n",
        "            title_span = title.find('span')\n",
        "            title_text = title_span.text if title_span is not None else \"N/A\"\n",
        "        else:\n",
        "            title_text = \"N/A\"\n",
        "\n",
        "        author = paper.find('a', class_='link person')\n",
        "        print (author)\n",
        "        if author == None or author == 'None':\n",
        "            continue\n",
        "        else:\n",
        "            author_span = author.find('span')\n",
        "            author_text = author_span.text if author_span is not None else \"N/A\"\n",
        "\n",
        "        date = paper.find('span', class_=\"date\")\n",
        "        date_text = date.text if date is not None else \"N/A\"\n",
        "\n",
        "        link = paper.find('h3', class_='title')\n",
        "        link_href = link.find('a', href=True)['href'] if link is not None else \"N/A\"\n",
        "        print(link_href)\n",
        "\n",
        "   #After this line, there is more code to append data to a DataFrame or perform additional operations.\n",
        "\n",
        "        #Retrieve data from the existing Crawling_database.csv file\n",
        "        opening = pd.read_csv('Crawling_database.csv', index_col=\"Unnamed: 0\")\n",
        "\n",
        "        #Generate a new DataFrame containing the data to be added\n",
        "        new_row = pd.DataFrame({'Title': [title.text],\n",
        "                                'Authors': [author.text],\n",
        "                                'Publication Year': [date.text],\n",
        "                                'Publication Link': [link_href]})\n",
        "\n",
        "        # Concatenate the existing DataFrame and the new row DataFrame\n",
        "        opening = pd.concat([opening, new_row], ignore_index=True)\n",
        "\n",
        "        # Save the updated DataFrame to database.csv\n",
        "        opening.to_csv('Crawling_database.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "accessory-customer",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:58:09.442618Z",
          "start_time": "2021-08-01T00:58:09.431490Z"
        },
        "id": "accessory-customer"
      },
      "outputs": [],
      "source": [
        "## Scrape function\n",
        "def scrape(mx):\n",
        "    df = append_to_csv('Crawling_database.csv')\n",
        "    i=0\n",
        "    while True:\n",
        "\n",
        "        if i > 17:\n",
        "            break\n",
        "\n",
        "        if i>0:\n",
        "            url = URL + '?page=' + str(i)\n",
        "        else:\n",
        "            url = URL\n",
        "\n",
        "        i = i+1\n",
        "\n",
        "        page = requests.get(url)\n",
        "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
        "        results = soup.find(id=\"main-content\")\n",
        "        researchers = results.find_all(\"li\", class_=\"grid-result-item\")\n",
        "\n",
        "        for researcher in researchers:\n",
        "            # Check if researcher has any papers\n",
        "            check = researcher.find('div', class_='stacked-trend-widget')\n",
        "            if check:\n",
        "                name = verify_department(researcher)\n",
        "                if name is None:\n",
        "                    pass\n",
        "                else:\n",
        "                    enter_each_researchers_publication(name, profile_url, df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "absent-vegetable",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:40.554457Z",
          "start_time": "2021-08-01T00:58:09.447859Z"
        },
        "id": "absent-vegetable"
      },
      "outputs": [],
      "source": [
        "generate_csv_file()\n",
        "append_to_csv(database='Crawling_database.csv') #Generate_csv\n",
        "\n",
        "%time scrape(max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "associate-subscription",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:40.569218Z",
          "start_time": "2021-08-01T00:59:40.557023Z"
        },
        "id": "associate-subscription",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7029a530-4f29-40cb-fd51-9fab6b999ad2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "237 records were scraped\n"
          ]
        }
      ],
      "source": [
        "test_db = pd.read_csv('Crawling_database.csv').rename(columns={'Unnamed: 0':'SN'})\n",
        "test_db\n",
        "print(f'{test_db.shape[0]} records were scraped')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "herbal-desktop",
      "metadata": {
        "id": "herbal-desktop"
      },
      "source": [
        "#Indexing Component: Efficient Data Retrieval System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "graphic-potter",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:40.600384Z",
          "start_time": "2021-08-01T00:59:40.572032Z"
        },
        "id": "graphic-potter"
      },
      "outputs": [],
      "source": [
        "crawled_db = pd.read_csv('Crawling_database.csv').rename(columns={'Unnamed: 0':'SN'}).reset_index(drop=True)\n",
        "crawled_db.head()\n",
        "# crawled_db = pd.read_csv('Crawling_database.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crawled_db.tail()"
      ],
      "metadata": {
        "id": "aQdqWnNa2g0P"
      },
      "id": "aQdqWnNa2g0P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "individual_row = crawled_db.loc[138,:].copy()\n",
        "individual_row"
      ],
      "metadata": {
        "id": "r9BLDxHz32LF"
      },
      "id": "r9BLDxHz32LF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawled_db.Title.unique()"
      ],
      "metadata": {
        "id": "kRz8Za8b4GHa"
      },
      "id": "kRz8Za8b4GHa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "crawled_db.Authors.value_counts()\n",
        "\n"
      ],
      "metadata": {
        "id": "gLVbHfkM2nMw"
      },
      "id": "gLVbHfkM2nMw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "marine-coordinate",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T07:22:25.795248Z",
          "start_time": "2021-08-01T07:22:25.784819Z"
        },
        "id": "marine-coordinate"
      },
      "outputs": [],
      "source": [
        "crawled_db.head(7)\n",
        "#ids = scraped_db[\"Title\"]\n",
        "#scraped_db[ids.isin(ids[ids.duplicated()])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "legendary-trauma",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:40.612357Z",
          "start_time": "2021-08-01T00:59:40.603287Z"
        },
        "id": "legendary-trauma"
      },
      "outputs": [],
      "source": [
        "individual_row = crawled_db.loc[138,:].copy()\n",
        "individual_row"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "valid-humor",
      "metadata": {
        "id": "valid-humor"
      },
      "source": [
        "#Text preprocessing / Data Analysis and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "quiet-train",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:40.647183Z",
          "start_time": "2021-08-01T00:59:40.615161Z"
        },
        "id": "quiet-train"
      },
      "outputs": [],
      "source": [
        "# Remove stop words\n",
        "sw = stopwords.words(\"english\")\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def tp1(txt):\n",
        "    txt = txt.lower()   # Make lowercase\n",
        "    txt = txt.translate(str.maketrans('','',string.punctuation))   # Remove punctuation marks\n",
        "    txt = lematize(txt)\n",
        "    return txt\n",
        "\n",
        "\n",
        "def fwpt(word):\n",
        "    tag = pos_tag([word])[0][1][0].upper()\n",
        "    hash_tag = {\"V\": wordnet.VERB, \"R\": wordnet.ADV,\"N\": wordnet.NOUN,\"J\": wordnet.ADJ}\n",
        "    return hash_tag.get(tag, wordnet.NOUN)\n",
        "\n",
        "def lematize(text):\n",
        "        tkns = nltk.word_tokenize(text)\n",
        "        ax = \"\"\n",
        "        for each in tkns:\n",
        "            if each not in sw:\n",
        "                ax += lemmatizer.lemmatize(each, fwpt(each)) + \" \"\n",
        "        return ax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fixed-triple",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:40.670745Z",
          "start_time": "2021-08-01T00:59:40.658588Z"
        },
        "id": "fixed-triple"
      },
      "outputs": [],
      "source": [
        "# crawl title, author\n",
        "individual_row[['Title', 'Authors']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "honey-strain",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:43.014244Z",
          "start_time": "2021-08-01T00:59:40.699046Z"
        },
        "id": "honey-strain"
      },
      "outputs": [],
      "source": [
        "# Illustration of converting text to lowercase and removing punctuation\n",
        "tp1(individual_row['Title'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "coordinated-columbus",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:43.027827Z",
          "start_time": "2021-08-01T00:59:43.016665Z"
        },
        "id": "coordinated-columbus"
      },
      "outputs": [],
      "source": [
        "#Example of lemmatization in action.\n",
        "\n",
        "lematize(tp1(individual_row['Title']))\n",
        "#lematize(individual_row['Title'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "equipped-retrieval",
      "metadata": {
        "id": "equipped-retrieval"
      },
      "source": [
        "#### Unprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "steady-snapshot",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T07:25:01.875323Z",
          "start_time": "2021-08-01T07:25:01.858252Z"
        },
        "id": "steady-snapshot"
      },
      "outputs": [],
      "source": [
        "crawled_db[['Title', 'Authors']].iloc[131]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "available-height",
      "metadata": {
        "id": "available-height"
      },
      "source": [
        "#### Processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vocal-burton",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T07:25:27.562359Z",
          "start_time": "2021-08-01T07:25:27.557862Z"
        },
        "id": "vocal-burton"
      },
      "outputs": [],
      "source": [
        "crawled_db[['Title','Authors']].iloc[120]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "biological-level",
      "metadata": {
        "id": "biological-level"
      },
      "source": [
        "#Preparing the DataFrame for Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "national-alexander",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:43.834681Z",
          "start_time": "2021-08-01T00:59:43.030064Z"
        },
        "id": "national-alexander"
      },
      "outputs": [],
      "source": [
        "processed_db = crawled_db.copy()\n",
        "\n",
        "def preprocess_df(df):\n",
        "    df.Title = df.Title.apply(tp1)\n",
        "    df.Author = df.Authors.str.lower()\n",
        "    df = df.drop(columns=['Authors','Publication Year'], axis=1)\n",
        "    return df\n",
        "\n",
        "preprocess_df(processed_db)\n",
        "processed_db.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "offshore-driver",
      "metadata": {
        "id": "offshore-driver"
      },
      "source": [
        "#Data Indexing and Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "insured-water",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:43.845672Z",
          "start_time": "2021-08-01T00:59:43.837012Z"
        },
        "id": "insured-water"
      },
      "outputs": [],
      "source": [
        "single = processed_db.loc[0,:].copy()\n",
        "print(single)\n",
        "indexing_trial = {}\n",
        "\n",
        "words = single.Title.split()\n",
        "SN = single.SN\n",
        "word = words[0]\n",
        "example = {word: [SN]}\n",
        "\n",
        "print('=====================================================================')\n",
        "print('Sample index')\n",
        "print(example)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crawled_db['Publication Link']"
      ],
      "metadata": {
        "id": "OnEqJUrf704p"
      },
      "id": "OnEqJUrf704p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Indexing Function Execution"
      ],
      "metadata": {
        "id": "bdm6cmfJ6FA7"
      },
      "id": "bdm6cmfJ6FA7"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "upper-flour",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:43.854491Z",
          "start_time": "2021-08-01T00:59:43.848459Z"
        },
        "id": "upper-flour"
      },
      "outputs": [],
      "source": [
        "#Indexing function\n",
        "def execute_indexing(inputs, index):\n",
        "    words = inputs.Title.split()\n",
        "    SN = int(inputs.SN)\n",
        "    for word in words:\n",
        "        if word in index.keys():\n",
        "            if SN not in index[word]:\n",
        "                index[word].append(SN)\n",
        "        else:\n",
        "            index[word] = [SN]\n",
        "    return index\n",
        "\n",
        "indx = execute_indexing(inputs=single, index= {})\n",
        "#print(indx)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Index Construction"
      ],
      "metadata": {
        "id": "eOT3geRI6KoT"
      },
      "id": "eOT3geRI6KoT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "knowing-amsterdam",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:44.790404Z",
          "start_time": "2021-08-01T00:59:43.857267Z"
        },
        "id": "knowing-amsterdam"
      },
      "outputs": [],
      "source": [
        "def full_index(df, index):\n",
        "    for x in range(len(df)):\n",
        "        inpt = df.loc[x,:]\n",
        "        ind = execute_indexing(inputs=inpt, index=index)\n",
        "    return ind\n",
        "\n",
        "def construct_index(df, index):\n",
        "    queue = preprocess_df(df)\n",
        "    ind = full_index(df=queue, index=index)\n",
        "    return ind\n",
        "\n",
        "indexed = full_index(processed_db,\n",
        "                     index = {})\n",
        "\n",
        "\n",
        "indexes = construct_index(df=crawled_db,\n",
        "                          index = {})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Index File Management"
      ],
      "metadata": {
        "id": "uzru9ybu6OM0"
      },
      "id": "uzru9ybu6OM0"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "presidential-piano",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T00:59:44.827243Z",
          "start_time": "2021-08-01T00:59:44.796816Z"
        },
        "id": "presidential-piano"
      },
      "outputs": [],
      "source": [
        "with open('index_data.json', 'w') as new_f:\n",
        "    json.dump(indexes, new_f, sort_keys=True, indent=4)\n",
        "\n",
        "with open('index_data.json', 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "def index_2(df, x_path):\n",
        "    if len(df) > 0:\n",
        "        with open(x_path, 'r') as file:\n",
        "            prior_index = json.load(file)\n",
        "        new_index = construct_index(df = df, index = prior_index)\n",
        "        with open(x_path, 'w') as new_f:\n",
        "            json.dump(new_index, new_f, sort_keys=True, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deluxe-jesus",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T10:41:54.049783Z",
          "start_time": "2021-08-01T10:41:53.953783Z"
        },
        "id": "deluxe-jesus"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pressing-editing",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T23:13:44.430869Z",
          "start_time": "2021-08-01T23:13:44.285366Z"
        },
        "id": "pressing-editing"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "simple-paintball",
      "metadata": {
        "id": "simple-paintball"
      },
      "source": [
        "#Query Handler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "vanilla-football",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T11:02:51.119226Z",
          "start_time": "2021-08-01T11:02:51.112657Z"
        },
        "id": "vanilla-football"
      },
      "outputs": [],
      "source": [
        "def show_query_processing():\n",
        "    sample = input('Please Input Search Terms: ')\n",
        "    processed_query = tp1(sample)\n",
        "    #print(f'User Search Query: {sample}')\n",
        "    print(f'Processed Search Query: {processed_query}')\n",
        "    return processed_query\n",
        "\n",
        "#show_query_processing()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "italian-turkish",
      "metadata": {
        "id": "italian-turkish"
      },
      "source": [
        "#Separate Query into Individual Terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "handed-client",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T11:03:01.618023Z",
          "start_time": "2021-08-01T11:02:59.088852Z"
        },
        "id": "handed-client",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2aa3356f-d822-4f80-aba6-1fd8fd4d7c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please Input Search Terms: detergentfree membrane protein purification\n",
            "Processed Search Query: detergentfree membrane protein purification \n",
            "Separate Query: ['detergentfree', 'membrane', 'protein', 'purification']\n"
          ]
        }
      ],
      "source": [
        "def separate_query(terms):\n",
        "    each = tp1(terms)\n",
        "    return each.split()\n",
        "\n",
        "dqp = show_query_processing()\n",
        "dqp\n",
        "print(f'Separate Query: {separate_query(dqp)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "loved-postage",
      "metadata": {
        "id": "loved-postage"
      },
      "source": [
        "#Boolean Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ready-denial",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T01:00:26.010007Z",
          "start_time": "2021-08-01T01:00:26.004950Z"
        },
        "id": "ready-denial"
      },
      "outputs": [],
      "source": [
        "def union(lists):\n",
        "    union = list(set.union(*map(set, lists)))\n",
        "    union.sort()\n",
        "    return union\n",
        "\n",
        "def intersection(lists):\n",
        "    intersect = list(set.intersection(*map(set, lists)))\n",
        "    intersect.sort()\n",
        "    return intersect"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "circular-distinction",
      "metadata": {
        "id": "circular-distinction"
      },
      "source": [
        "#Search Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "finite-pottery",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T01:00:26.020938Z",
          "start_time": "2021-08-01T01:00:26.012920Z"
        },
        "id": "finite-pottery"
      },
      "outputs": [],
      "source": [
        "def vertical_search_handler(df, query, index=indexes):\n",
        "    query_separate = separate_query(query)\n",
        "    retrieved = []\n",
        "    for word in query_separate:\n",
        "        if word in index.keys():\n",
        "            retrieved.append(index[word])\n",
        "\n",
        "\n",
        "    # Ranked Retrieval\n",
        "    if len(retrieved)>0:\n",
        "        high_rank_result = intersection(retrieved)\n",
        "        low_rank_result = union(retrieved)\n",
        "        c = [x for x in low_rank_result if x not in high_rank_result]\n",
        "        high_rank_result.extend(c)\n",
        "        result = high_rank_result\n",
        "\n",
        "        final_output = df[df.SN.isin(result)].reset_index(drop=True)\n",
        "\n",
        "        # Return result in order of Intersection ----> Union\n",
        "        dummy = pd.Series(result, name = 'SN').to_frame()\n",
        "        result = pd.merge(dummy, final_output, on='SN', how = 'left')\n",
        "\n",
        "    else:\n",
        "        result = 'No result found'\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test Search Function"
      ],
      "metadata": {
        "id": "WRqAgqNABOZl"
      },
      "id": "WRqAgqNABOZl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "facial-killer",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T01:00:37.508501Z",
          "start_time": "2021-08-01T01:00:26.023723Z"
        },
        "id": "facial-killer"
      },
      "outputs": [],
      "source": [
        "def test_search_engine():\n",
        "    xtest = crawled_db.copy()\n",
        "    query = input(\"Please provide your search query: \")\n",
        "    return vertical_search_handler(xtest, query, indexed)\n",
        "\n",
        "test_search_engine()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Search Function"
      ],
      "metadata": {
        "id": "ztgdy9XoBSEA"
      },
      "id": "ztgdy9XoBSEA"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "accredited-crazy",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T11:43:05.137930Z",
          "start_time": "2021-08-01T11:43:05.131660Z"
        },
        "id": "accredited-crazy"
      },
      "outputs": [],
      "source": [
        "def final_search_engine(results):\n",
        "    if type(results) != 'list':\n",
        "        return results\n",
        "        #print(results)\n",
        "    else:\n",
        "        for i in range(len(results)):\n",
        "            printout = results.loc[i, :]\n",
        "            #print(printout['Title'])\n",
        "            #print(printout['Authors'])\n",
        "            #print(printout['Publication Year'])\n",
        "            #print(printout['Publication Link'])\n",
        "            #print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "urban-astrology",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T11:46:39.943876Z",
          "start_time": "2021-08-01T11:46:39.927460Z"
        },
        "id": "urban-astrology"
      },
      "outputs": [],
      "source": [
        "crawled_db['Authors'].iloc[24]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "speaking-savings",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T11:44:03.697525Z",
          "start_time": "2021-08-01T11:43:54.760859Z"
        },
        "id": "speaking-savings"
      },
      "outputs": [],
      "source": [
        "final_search_engine(test_search_engine())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "first-toilet",
      "metadata": {
        "id": "first-toilet"
      },
      "source": [
        "## 4. Schedule Crawler for every week or CronJob\n",
        "\n",
        "To demonstrate a weekly scheduled crawling, the following parameters are defined:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cutting-prayer",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2021-08-01T01:05:57.578325Z",
          "start_time": "2021-08-01T01:00:37.523485Z"
        },
        "id": "cutting-prayer"
      },
      "outputs": [],
      "source": [
        "# days = 0\n",
        "# interval = 7\n",
        "# while days <= 1:\n",
        "#     scrape(max)\n",
        "#     print(f\"Crawled at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "#     print(f'Next crawl scheduled after {interval} days')\n",
        "#     time.sleep(interval)\n",
        "#     days = days + 1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}